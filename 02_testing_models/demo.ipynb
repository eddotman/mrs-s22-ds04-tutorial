{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Testing ML Models\n",
    "\n",
    "In this tutorial notebook, we'll examine some examples of why we may want to test models, and how this can be done.\n",
    "\n",
    "In this session, the focus will mostly be on code design and analysis, and discussing the benefits and limits of testing models before using them for \"real\" predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some imports -- we will use the `inspect` library to print out some imported code in-line, and the imported code represents some custom ML model tests that your lab's research team has written.\n",
    "\n",
    "**Q: If you didn't want to print everything out in the notebook, where would you look for this code? What are the tradeoffs of inspecting code in different environments or IDEs?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getsource\n",
    "\n",
    "import classic_tests\n",
    "import physicality_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What does this following test do? Why might we write it and what intuition are we capturing?**\n",
    "\n",
    "**Q: What does the \"parametrize\" function decorator do?**\n",
    "\n",
    "**Q: What would you think if a colleague wrote some code and caused this test to fail?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@pytest.mark.parametrize(\n",
      "    \"material\",\n",
      "    [\n",
      "        \"Li7La3(SnO6)2\",\n",
      "        \"Li3(WO3)8\",\n",
      "        \"Li4Mn2P4H3O16\",\n",
      "        \"Li2Ni(PO3)5\",\n",
      "        \"MgV2O4\",\n",
      "    ]\n",
      ")\n",
      "def test_prediction_bounds(material:str, model:SynthesizabilityModel) -> None:\n",
      "    predicted_value = model.predict_single(material)\n",
      "    assert 0 <= predicted_value <= 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(getsource(classic_tests.test_prediction_bounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Let's discuss the same points, looking at the next test.**\n",
    "\n",
    "**Q: Is this test hinting at a different intuition? What would you think if some new code caused this test to fail?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@pytest.mark.parametrize(\n",
      "    \"material\",\n",
      "    [\n",
      "        1234,\n",
      "        dict({\"a\":\"b\"}),\n",
      "        [\"Fe2O3\"],\n",
      "        [\"Fe2\", \"O3\"],\n",
      "    ]\n",
      ")\n",
      "def test_invalid_material(material:str, model:SynthesizabilityModel) -> None:\n",
      "    with pytest.raises(TypeError):\n",
      "        model.predict_single(material)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(getsource(classic_tests.test_invalid_material))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Let's switch to a less standard type of testing: can we use software tests to understand the behaviours of our ML models?**\n",
    "\n",
    "**Q: What intuition is this test capturing? Could anyone write this test, or do you need special domain knowledge? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@pytest.mark.parametrize(\n",
      "    \"input_series\",\n",
      "    [\n",
      "        [\n",
      "            \"MgV2O4\",\n",
      "            \"O4MgV2\",\n",
      "            \"V2MgO4\",\n",
      "        ],\n",
      "        [\n",
      "            \"Li3(WO3)8\",\n",
      "            \"(WO3)8Li3\",\n",
      "        ]\n",
      "    ]\n",
      ")\n",
      "@pytest.mark.xfail(reason=\"Too hard for a random model!\")\n",
      "def test_atomic_order(input_series:str) -> None:\n",
      "    predicted_values = [\n",
      "        synthesizability_model.predict_single(input_val)\n",
      "        for input_val in input_series\n",
      "    ]\n",
      "\n",
      "    assert max(predicted_values) - min(predicted_values) < 0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(getsource(physicality_tests.test_atomic_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What about this next test? Is the intuition that is tested reasonable? Why or why not?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@pytest.mark.parametrize(\n",
      "    \"input_series\",\n",
      "    [\n",
      "        [\n",
      "            \"MgV2O4\",\n",
      "            \"MgV8O4\",\n",
      "            \"MgV24O4\",\n",
      "        ],\n",
      "        [\n",
      "            \"Li3(WO3)8\",\n",
      "            \"Li3(W5O3)8\",\n",
      "            \"Li3(W20O3)8\",\n",
      "        ]\n",
      "    ]\n",
      ")\n",
      "@pytest.mark.xfail(reason=\"Too hard for a random model!\")\n",
      "def test_stoichiometric_monotonicity(input_series:str) -> None:\n",
      "    predicted_values = [\n",
      "        synthesizability_model.predict_single(input_val)\n",
      "        for input_val in input_series\n",
      "    ]\n",
      "\n",
      "    assert predicted_values == sorted(predicted_values, reverse=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(getsource(physicality_tests.test_stoichiometric_monotonicity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: To prove the point about automation and saving time: looking at the test executions below, what do you think about the opportunities for time-saved? Is it worth the tradeoff of writing tests in all cases?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.7.6, pytest-7.1.0, pluggy-0.13.1\n",
      "rootdir: /Users/eddie/Documents/GitHub/mrs-s22-ds04-tutorial/02_testing_models\n",
      "plugins: hypothesis-5.5.4, arraydiff-0.3, remotedata-0.3.2, openfiles-0.4.0, doctestplus-0.5.0, astropy-header-0.1.2\n",
      "collected 9 items                                                              \u001b[0m\n",
      "\n",
      "classic_tests.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m9 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest classic_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.7.6, pytest-7.1.0, pluggy-0.13.1\n",
      "rootdir: /Users/eddie/Documents/GitHub/mrs-s22-ds04-tutorial/02_testing_models\n",
      "plugins: hypothesis-5.5.4, arraydiff-0.3, remotedata-0.3.2, openfiles-0.4.0, doctestplus-0.5.0, astropy-header-0.1.2\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "physicality_tests.py \u001b[33mx\u001b[0m\u001b[33mx\u001b[0m\u001b[33mx\u001b[0m\u001b[33mx\u001b[0m\u001b[33m                                                [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== \u001b[33m\u001b[1m4 xfailed\u001b[0m\u001b[33m in 0.04s\u001b[0m\u001b[33m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest physicality_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
